In this section, we will first state our problem in a probabilistic manner and
present the mathematical groundings of our method. A practical algorithmic
sketch will conclude the presentation.

\subsection{Problem Formulation\label{subsec:prob}}

In the following, we borrow the formalism of the probabilistic discrete-time
Simultaneous Localization and Mapping (SLAM)
model~\cite{durrantwhyte06simultaneous}. For the sake of clarity, we
consider here a robot with a single sensor observing a known number of landmarks
at each timestep. Furthermore, we assume the correspondences between sensor's
measurements and landmarks are known. While this latter assumption involves data
association techniques which go beyond the scope of this paper, the extension to
multiple sensors is straightforward.

Let
$\mathcal{X}=\{\mathbf{x}_{0:K}\}$ be a set of \emph{latent} random variables
(LRV) representing robot states up to timestep $K$,
$\mathcal{U}=\{\mathbf{u}_{1:K}\}$ a set of \emph{observable} random variables
(ORV) representing measured control inputs,
$\mathcal{L}=\{\boldsymbol{\ell}_{1:N}\}$ a set of LRV representing $N$
landmark positions, $\mathcal{Z}=\{\mathbf{z}_{1_{1:N}:K_{1:N}}\}$ a set of ORV
representing $K \times N$ landmark measurements, and $\boldsymbol{\Theta}$, an
LRV representing the calibration parameters of the robot's sensor. The goal of
the calibration procedure is to compute the posterior marginal distribution of
$\boldsymbol{\Theta}$ given all the measurements up to timestep $K$,

\begin{equation}\label{eqn:post_calib}
  \begin{aligned}
  p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z}) &=
    \int_{\mathcal{X}, \mathcal{L}}p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

\noindent Following~\cite{durrantwhyte06simultaneous}, the full joint posterior
on the right-hand side of \eqref{eqn:post_calib} may further be factorized into

\begin{multline}\label{eqn:post_joint_factorized}
  p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}) \propto\\
    p(\boldsymbol{\Theta}, \mathbf{x}_0,\mathcal{L})
    \prod_{k=1}^K p(\mathbf{x}_k\mid\mathbf{x}_{k - 1},\mathbf{u}_k)
    \prod_{k=1}^K\prod_{i=1}^N p(\mathbf{z}_{k_i}\mid\mathbf{x}_k,
    \mathbf{l}_i,\boldsymbol{\Theta}).
\end{multline}

\noindent We may then approximate \eqref{eqn:post_joint_factorized} with a
normal distribution whose mean, $\boldsymbol{\mu}_{\boldsymbol{\Theta}
\mathcal{X}\mathcal{L}}$, and covariance, $\boldsymbol{\Sigma}_
{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$, have to be estimated. To this end,
we first derive a Maximum a Posteriori (MAP) estimator for the mean,

\begin{equation}\label{eqn:map_estimator}
  \begin{aligned}
  \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \argmax_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z})
    \\
    &= \argmin_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}-\log
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

We further refine our problem by defining a \emph{motion} and \emph{observation}
model

\begin{equation}\label{eqn:process_model}
  \begin{aligned}
  \mathbf{x}_k &= \mathbf{h}(\mathbf{x}_{k-1}, \mathbf{u}_k, \mathbf{w}_k)\\
  \mathbf{z}_{k_i} &= \mathbf{g}(\mathbf{x}_{k}, \boldsymbol{\ell}_i,
    \boldsymbol{\Theta}, \mathbf{n}_k),
  \end{aligned}
\end{equation}

\noindent where

\begin{equation}\label{eqn:noise_model}
  \begin{aligned}
  \mathbf{w}_k \sim \mathcal{N}(\mathbf{0},\mathbf{Q}_k)\\
  \mathbf{n}_k \sim \mathcal{N}(\mathbf{0},\mathbf{R}_k)
  \end{aligned}
\end{equation}

\noindent are normally distributed process and observation noise variables,
$\mathbf{Q}_k$ and $\mathbf{R}_k$ being known covariance matrices. Although, the
functions $\mathbf{h}(\cdot)$ and $\mathbf{g}(\cdot)$ might be non-linear in
their parameters, we can approximate
$p(\mathbf{x}_k\mid\mathbf{x}_{k - 1},\mathbf{u}_k)$ and
$p(\mathbf{z}_{k_i}\mid\mathbf{x}_k, \mathbf{l}_i,\boldsymbol{\Theta})$ as
normal distributions through linearization.

\subsection{Least Squares Solution}

In case of linear motion and observation models, there exists a closed-form
solution to \eqref{eqn:map_estimator} based on the \emph{least squares} method
due to the normally distributed noise variables. In the other case, one can
resort to non-linear least squares methods that iteratively solve a linearized
version of the problem. In the following, we employ the \emph{Gauss-Newton}
algorithm~\cite{aster11parameter} for this purpose.

From \eqref{eqn:map_estimator} and the approximation that the densities involved
are normal, we can turn the MAP problem into the minimization of a sum of
quadratic functions. Non-linear optimization techniques start with an initial
guess,
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$, and refine
it iteratively with
$\delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ until
convergence.
$\delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ is
chosen in such a way that it minimizes a quadratic approximation of the
objective function around
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.
Gauss-Newton method only requires the stacked Jacobian matrix of the error
terms, $\mathbf{H}$. In block
matrix form, the update takes the form
\begin{equation}\label{eqn:dx_update}
  \begin{aligned}
  (\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{H}^T\mathbf{T}^{-1}\mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}
where $\mathbf{T}$ is formed with $\mathbf{Q}_k$ and $\mathbf{N}_k$, and
$\mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}})$ is the error of the current
estimate $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.

At convergence of the algorithm, the quantity
$\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H}$ is the \emph{Fisher Information Matrix}
(FIM) and the inverse covariance matrix
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ of
our estimate
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.

If we let $\mathbf{T}^{-1}=\mathbf{L}^T\mathbf{L}$ be the Cholesky
decomposition of the precision matrix, \eqref{eqn:dx_update} can be rewritten as
\begin{equation}\label{eqn:dx_update_normal}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})^T(\mathbf{L}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &= 
    -(\mathbf{L}\mathbf{H})^T\mathbf{L}
    \mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

\noindent which we can recognize as the \emph{normal equations} of the linear
system

\begin{equation}\label{eqn:dx_update_standard}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{L}\mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}).
  \end{aligned}
\end{equation}

Thus, instead of directly solving \eqref{eqn:dx_update} which requires a
significant amount of computation and introduces numerical errors, we can take
advantage of matrix decompositions.

Let $\mathbf{L}\mathbf{H}$ be of size $m\times n$ with the following thin
Singular Value Decomposition (\emph{SVD}) decomposition~\cite{golub96matrix}

\begin{equation}\label{eqn:svd_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H} &= \mathbf{U}_n\mathbf{S}_n\mathbf{V}_n^T,
  \end{aligned}
\end{equation}

\noindent where $\mathbf{U}_n$ is an $m\times n$ matrix with orthogonal columns,
$\mathbf{S}_n=\diag(\sigma_1, \cdots,\sigma_n)$, $\mathbf{V}_n$ an $n\times n$
matrix with orthogonal columns, and $\sigma_i$ are the
singular values of $\mathbf{L}\mathbf{H}$.

From \eqref{eqn:svd_decomposition} and using the orthogonality of
$\mathbf{U}_n$ and $\mathbf{V}_n$, we can solve \eqref{eqn:dx_update} as

\begin{equation}\label{eqn:dx_svd_solve}
  \begin{aligned}
  \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}^
    {(SVD)} &=
    -\mathbf{V}_n\mathbf{S}_n^{-1}\mathbf{U}_n^T\mathbf{L}
    \mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}).
  \end{aligned}
\end{equation}

Although useful for illustrating the concepts of our approach, the SVD
decomposition can be computationally demanding for large matrices. In practice,
we therefore use the thin \emph{QR} decomposition~\cite{golub96matrix} of
$\mathbf{L}\mathbf{H}$

\begin{equation}\label{eqn:qr_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H}\boldsymbol{\Pi} &= \mathbf{Q}_n\mathbf{R}_n,
  \end{aligned}
\end{equation}

\noindent where $\boldsymbol{\Pi}$ is a permutation matrix, $\mathbf{Q}_n$ an
$m\times n$ matrix with orthogonal columns, and $\mathbf{R}_n$ an $n\times n$
upper triangular matrix. In standard QR decomposition, $\boldsymbol{\Pi}$ is
the identity matrix, otherwise it reflects column pivoting.

From \eqref{eqn:qr_decomposition} and using the orthogonality of $\mathbf{Q}_n$,
\eqref{eqn:dx_update} can be expressed as

\begin{equation}\label{eqn:dx_qr_solve}
  \begin{aligned}
  \mathbf{R}_n\boldsymbol{\Pi}^{T}
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}^
    {(QR)}&=
    -\mathbf{Q}_n^T \mathbf{L}\mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

\noindent which, due the upper triangular form of $\mathbf{R}_n$, can be easily
solved by \emph{back substitution}.

The $\mathbf{R}_n$ matrix can also be used to compute the FIM and its
inverse, the covariance estimate
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$. If we
drop the permutation matrix for clarity, the FIM becomes
$\mathbf{R}_n^T\mathbf{R}_n$. Let $\sigma_{ij}$ denote the elements of
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ and
$r_{ij}$ the elements of $\mathbf{R}_n$, there exists an
efficient algorithm~\cite{kaess09covariance} for recovering any elements of the
covariance estimate without inverting the whole FIM

\begin{equation}\label{eqn:covariance_QR}
  \begin{aligned}
  \sigma_{ll} &= \frac{1}{r_{ll}}(\frac{1}{r_{ll}} -
    \sum_{j = l + 1, r_{lj}\neq 0}^n r_{lj}\sigma_{jl})\\
  \sigma_{il} &= \frac{1}{r_{ii}}
    (-\sum_{j = i + 1, r_{ij}\neq 0}^n r_{ij}\sigma_{jl} -
    \sum_{j = l + 1, r_{ij}\neq 0}^n r_{ij}\sigma_{lj}),
  \end{aligned}
\end{equation}

\noindent for $l=n,\cdots,1$, $i=l-1,\cdots,1$ and the lower part is given by
symmetry.

At the convergence of the Gauss-Newton optimization, we are thus left with the
estimates $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$
and $\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ of
the normal distribution $p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L}
\mid\mathcal{U},\mathcal{Z})$. In order to solve \eqref{eqn:post_calib}, we can
employ the marginalization property of normal
distributions~\cite{bishop06pattern}. If we express the estimates in the
partitioned form

\begin{equation}\label{eqn:partitioned_estimates}
  \begin{aligned}
  \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \begin{pmatrix}
    \hat{\boldsymbol{\mu}}_{\mathcal{X}}\\
    \hat{\boldsymbol{\mu}}_{\mathcal{L}}\\
    \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}
    \end{pmatrix},
  \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \begin{pmatrix}
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\boldsymbol{\Theta}}\\
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\boldsymbol{\Theta}}\\
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}\\
    \end{pmatrix},
  \end{aligned}
\end{equation}

\noindent then $p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z})\sim\mathcal{N}
(\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}},
\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}})$. We can
thus extract $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}$ from
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ very
efficiently, and by placing $\boldsymbol{\Theta}$ to the right of the Jacobian
matrix $\mathbf{H}$ and using \eqref{eqn:covariance_QR},
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}$ can be
computed in $\mathcal{O}(l)$, where $l=\card(\boldsymbol{\Theta})$.

\subsection{Truncated SVD and QR Solutions}

There exists a solution to \eqref{eqn:dx_update}
iff the FIM is invertible, i.e., it is \emph{full rank}. The rank of a
matrix is the maximum number of linearly independent column or row vectors. A
matrix with $m$ rows and $n$ columns has full rank when its rank is equal to
$\min(m,n)$. In real-world scenarios, where data is contaminated by noise, the
FIM might appear to be theoretically full rank although it is actually
\emph{rank deficient}. We can illustrate this with the matrix

\begin{equation}\label{eqn:rank_deficient_matrix}
  \begin{aligned}
    \mathbf{A} &=
    \begin{pmatrix}
    0.9999&1.9999&3.0014\\
    4.0007&5.0015&6.0007\\
    6.9998&8.0014&8.9988
    \end{pmatrix}.
  \end{aligned}
\end{equation}

Although $\mathbf{A}$ is technically full rank, it was constructed from a rank
deficient matrix (the second row is a linear combination of the first and the
last row) with added Gaussian noise. In the following, we call $\mathbf{A}$
a \emph{numerically rank deficient} matrix. The issue of rank deficiency often
appears in robotics state estimation and is related to the problem of
\emph{observability} in control theory~\cite{jauffret07observability}.

Using SVD decomposition, we can identify a numerically rank deficient matrix by
analyzing its singular values \cite{hansen98rank}. The \emph{numerical rank}
$r$ of a matrix is defined as the index of its smallest singular value
$\sigma_r$ larger than a user-defined tolerance $\epsilon$, i.e.,

\begin{equation}\label{eqn:numerical_rank}
  \begin{aligned}
  r &= \argmax_i \sigma_i\geq\epsilon.
  \end{aligned}
\end{equation}

For example, the singular values of $\mathbf{A}$ are approximately 16.8, 1.1,
and 0.0001. From \eqref{eqn:dx_svd_solve}, we can see that if some of the
$\sigma_i$ are close to zero, i.e. $r<n$, the update vector
$\delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}^
{(SVD)}$ will be large and eventually lead to a divergence of the solution. In
order to cope with this issue, we can approximate $\mathbf{L}\mathbf{H}$ with a
lower rank matrix yielding the \emph{Truncated} SVD
(TSVD)~\cite{varah73numerical} solution

\begin{equation}\label{eqn:dx_tsvd_solve}
  \begin{aligned}
  \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}^
    {(TSVD)} &=
    -\mathbf{V}_r\mathbf{S}_r^{-1}\mathbf{U}_r^T
    \mathbf{L}\mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

\noindent with $\mathbf{U}_r$ an $m\times r$ matrix,
$\mathbf{S}_r=\diag(\sigma_1, \cdots,\sigma_r)$, $\mathbf{V}_r$ an $n\times r$
matrix.

It is worth noting that TSVD applies a \emph{sharp} filter to the system,
whereas \emph{regularization} methods such as Tikhonov or ridge regression work
as a \emph{smooth} filter. As shown in~\cite{hansen87truncated}, it produces
similar results at lower computational costs.

A similar approach can be derived using \emph{rank-revealing} QR
decomposition~\cite{hong92rank} yielding to the Truncated QR (TQR)
method~\cite{kitagawa01regularization}. The strategy is to apply QR
factorization with column pivoting in order to reveal the rank and to apply a
cutoff on the diagonal of the $\mathbf{R}$ matrix. It can further be noted that
the $\mathbf{Q}$ matrix need not be explicitly constructed, but instead returned
in the form of \emph{Householder} reflections that can be applied on the
right-hand side of the linear system.

\subsection{Selecting Informative Measurements}

Since we are mainly interested in calibrating our sensor and thus in the
posterior $p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z})$, we do not need
to consider all measurements. Let us define a partition of the measurements
$\mathcal{D}_1=\{\mathbf{u}_{1:i}, \mathbf{z}_{1_{1:N}:i_{1:N}}\}$ and
$\mathcal{D}_2=\{\mathbf{u}_{i+1:K}, \mathbf{z}_{i+1_{1:N}:K_{1:N}}\}$ with
$i<K$. Using information theory, we can quantify the information gain when
estimating $\boldsymbol{\Theta}$ with $\mathcal{D}_1$ alone or with
$\mathcal{D}_1$ and $\mathcal{D}_2$.

The \emph{mutual information} (MI)~\cite{mackay05information} between two random
variables $\mathbf{X}$ and $\mathbf{Y}$ is defined as

\begin{equation}\label{eqn:mi_definition}
  \begin{aligned}
  I(\mathbf{X};\mathbf{Y}) &=
    \int_{\mathbf{X}}\int_{\mathbf{Y}}p(x, y)\log\frac{p(x,y)}{p(x)p(y)},
  \end{aligned}
\end{equation}

\noindent where $p(x, y)$ is the joint density of $\mathbf{X}$ and $\mathbf{Y}$,
and $p(x)$ and $p(x)$ their marginal densities. The MI measures the amount of
information $\mathbf{X}$ and $\mathbf{Y}$ share or, in other words, quantifies
the reduction of uncertainty in $\mathbf{X}$ when knowing $\mathbf{Y}$.

Before deriving the MI for our purpose, it is worth recalling another property
of normal distributions~\cite{bishop06pattern}. If we define
$\mathbf{\Theta}_1=\mathbf{\Theta}\mid\mathcal{D}_1$ and consider the joint
normal distribution $p(\mathbf{\Theta}_1,\mathcal{D}_2)$ with parameters

\begin{equation}\label{eqn:joint_normal}
  \begin{aligned}
  \boldsymbol{\mu}_{\boldsymbol{\Theta}_1\mathcal{D}_2} &=
    \begin{pmatrix}
    \boldsymbol{\mu}_{\boldsymbol{\Theta}_1}\\
    \boldsymbol{\mu}_{\mathcal{D}_2}\\
    \end{pmatrix},
  \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2} &=
    \begin{pmatrix}
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\boldsymbol{\Theta}_1}&
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2}\\
    \boldsymbol{\Sigma}_{\mathcal{D}_2\boldsymbol{\Theta}_1}&
    \boldsymbol{\Sigma}_{\mathcal{D}_2\mathcal{D}_2}
    \end{pmatrix},
  \end{aligned}
\end{equation}

\noindent then $p(\mathbf{\Theta}_1\mid\mathcal{D}_2)$ is a normal distribution
with parameters

\begin{equation}\label{eqn:joint_conditional}
  \begin{aligned}
  \boldsymbol{\mu}_{\boldsymbol{\Theta}_1\mid\mathcal{D}_2} &=
    \boldsymbol{\mu}_{\boldsymbol{\Theta}_1} +
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2}
    \boldsymbol{\Sigma}_{\mathcal{D}_2\mathcal{D}_2}^{-1}
    (\mathcal{D}_2 - \boldsymbol{\mu}_{\mathcal{D}_2})\\
  \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mid\mathcal{D}_2} &=
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\boldsymbol{\Theta}_1} -
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2}
    \boldsymbol{\Sigma}_{\mathcal{D}_2\mathcal{D}_2}^{-1}
    \boldsymbol{\Sigma}_{\mathcal{D}_2\boldsymbol{\Theta}_1}.
  \end{aligned}
\end{equation}

Furthermore, $\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mid\mathcal{D}_2}$ can
be recognized as the \emph{Schur complement} of the joint covariance matrix
$\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2}$.

Following an argument of~\cite{davison05active}, the MI between
$\mathbf{\Theta}_1$ and $\mathcal{D}_2$ can then be computed as

\begin{equation}\label{eqn:mi_normal}
  \begin{aligned}
  I(\mathbf{\Theta}_1;\mathcal{D}_2) &=
    \mathbb{E}\left[\log
    \frac{p(\mathbf{\Theta}_1\mid\mathcal{D}_2)}{p(\mathbf{\Theta}_1)}\right]\\
    &= \frac{1}{2}\log\frac{|\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1
    \boldsymbol{\Theta}_1}|}
    {|\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\boldsymbol{\Theta}_1} -
    \boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mathcal{D}_2}
    \boldsymbol{\Sigma}_{\mathcal{D}_2\mathcal{D}_2}^{-1}
    \boldsymbol{\Sigma}_{\mathcal{D}_2\boldsymbol{\Theta}_1}|}\\
    &= \frac{1}{2}\log\frac{|\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1
    \boldsymbol{\Theta}_1}|}
    {|\boldsymbol{\Sigma}_{\boldsymbol{\Theta}_1\mid\mathcal{D}_2}|},
  \end{aligned}
\end{equation}

\noindent where $|\cdot|$ denotes the matrix determinant.

Thus, using \eqref{eqn:mi_normal}, we can measure the amount of information
$\mathcal{D}_2$ conveys to our current estimate
$\boldsymbol{\Theta}\mid\mathcal{D}_1$.

\subsection{Algorithm}

Thus far, we have delivered a formal introduction to the probabilistic
groundings of our self-supervised calibration approach. This section will be
dedicated to a practical online implementation of our algorithm.

The proposed calibration method is sketched in Alg.~\ref{alg:calibration}.
Defining $\mathcal{D}^{info}$ as the set of \emph{informative} measurements at
time $t$, we collect a measurement batch
$\mathcal{D}^{new}=\{\mathbf{u}_{t:t+k}, \mathbf{z}_{t_{1:N}:{t+k}_{1:N}}\}$
during $k$ timesteps and compute
$p(\mathbf{\Theta}\mid\mathcal{D}^{info},\mathcal{D}^{new})$ using Gauss-Newton
method with TQR updates. In a second step, if
$I(\mathbf{\Theta}\mid\mathcal{D}^{info};\mathcal{D}^{new})$ is larger
than a user-defined threshold $\theta$, $\mathcal{D}^{new}$ is added to
$\mathcal{D}^{info}$ and the estimate of $\mathbf{\Theta}$ is updated.

\begin{algorithm}[ht]
\caption{calibrateSensor()}
\label{alg:calibration}
%\dontprintsemicolon
\SetKwComment{Comment}{// }{}
\KwIn{Initial guesses $\hat{\boldsymbol{\Theta}}^{(0)}$,
  $\hat{\mathbf{x}}_0^{(0)}$, $\hat{\mathcal{L}}^{(0)}$}
\KwIn{Motion $\mathbf{h}(\cdot)$ and observation $\mathbf{g}(\cdot)$ models}
\KwIn{Batch size $k$, TQR threshold $\epsilon$, MI threshold $\theta$}
\KwOut{$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}$ and
  $\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}$}
\vskip 0.2cm
$\mathcal{D}^{info}\leftarrow\emptyset$ \;
$t\leftarrow1$ \;
\While {calibrate} {
  \Comment*[h]{Collecting measurements} \;
  $t^{init} \leftarrow t$ \;
  $\mathcal{D}^{new}\leftarrow\emptyset$ \;
  \While {$t < t^{init} + k$} {
    $\hat{\mathbf{x}}_t^{(0)} \leftarrow
      \mathbf{h}(\hat{\mathbf{x}}_{t-1}^{(0)},\mathbf{u}_t,\mathbf{0})$ \;
    $\mathcal{D}^{new}\leftarrow\mathcal{D}^{new}\cup\{\mathbf{u}_t,
      \mathbf{z}_{t_{1:N}}\}$ \;
    $t \leftarrow t + 1$ \;
  }
  \Comment*[h]{TQR minimization} \;
  $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} \leftarrow
    \argmax_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{D}^{info},
    \mathcal{D}^{new})$ \;
  \Comment*[h]{Marginalization} \;
  $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}\leftarrow
    \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ \;
  $\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}\leftarrow
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ \;
  \Comment*[h]{MI decision} \;
  \If {$I(\mathbf{\Theta}\mid\mathcal{D}^{info};\mathcal{D}^{new})>\theta$} {
    $\mathcal{D}^{info}\leftarrow\mathcal{D}^{info}\cup\mathcal{D}^{new}$ \;
    $\boldsymbol{\Theta}\sim\mathcal{N}(
      \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}},
      \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}})$ \;
  }
}
\end{algorithm}

Our method has been fully implemented in MATLAB. Taking into account the
sparsity of the Jacobian matrix $\mathbf{H}$, the algorithmic complexity can
be largely reduced in terms of memory footprint and computation. To this end, we
have used the SuiteSparseQR~\cite{davis11algorithm} package for performing
TQR optimization.
