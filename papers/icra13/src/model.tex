\subsection{Problem Formulation\label{subsec:prob}}

In the following, we outline our problem in a probabilistic manner, closely
following the discrete-time Simultaneous Localization and Mapping (SLAM)
formulation~\cite{durrantwhyte06simultaneous}. For the sake of clarity, we
consider here a robot with a single sensor observing a known number of landmarks
at each timestep. Furthermore, we assume the correspondences between sensor's
measurements and landmarks are known. The relaxation of these assumptions goes
beyond the scope of this paper.

Let
$\mathcal{X}=\{\mathbf{x}_{0:K}\}$ be a set of \emph{latent} random variables
(LRV) representing robot states up to timestep $K$,
$\mathcal{U}=\{\mathbf{u}_{1:K}\}$ a set of \emph{observable} random variables
(ORV) representing measured control inputs, $\mathcal{L}=\{\mathbf{l}_{1:N}\}$ a
set of LRV representing $N$ landmarks' positions,
$\mathcal{Z}=\{\mathbf{z}_{1_{1:N}:K_{1:N}}\}$ a set of ORV representing $KxN$
landmarks' measurements, and $\boldsymbol{\Theta}$ a LRV representing the
calibration parameters of the robot's sensor. The goal of the calibration
procedure is to compute the posterior marginal distribution of
$\boldsymbol{\Theta}$ given all the measurements up to timestep $K$

\begin{equation}\label{eqn:post_calib}
  \begin{aligned}
  p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z}) &=
    \int_{\mathcal{X}, \mathcal{L}}p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

The full joint posterior on the left-hand side of \eqref{eqn:post_calib} may
further be factorized into

\begin{equation}\label{eqn:post_joint_factorized}
  \begin{aligned}
  p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}) &\propto\\
    p(\boldsymbol{\Theta}, \mathbf{x}_0,\mathcal{L})
    \prod_{k=1}^K p(\mathbf{x}_k\mid\mathbf{x}_{k - 1},\mathbf{u}_k)
    \prod_{k=1}^K\prod_{i=1}^N p(\mathbf{z}_{k_i}\mid\mathbf{x}_k,
    \mathbf{l}_i,\boldsymbol{\Theta}).
  \end{aligned}
\end{equation}

We may approximate \eqref{eqn:post_joint_factorized} with a normal distribution
whose moments $\boldsymbol{\mu}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ and
$\boldsymbol{\Sigma}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ have to be
estimated. To this end, we first derive a Maximum a Posteriori (MAP) estimator
for the mean

\begin{equation}\label{eqn:map_estimator}
  \begin{aligned}
  \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \argmax_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z})
    \\
    &= \argmin_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}-\log
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

We further refine our problem by defining a \emph{motion} and \emph{observation}
model

\begin{equation}\label{eqn:process_model}
  \begin{aligned}
  \mathbf{x}_k &= \mathbf{h}(\mathbf{x}_{k-1}, \mathbf{u}_k, \mathbf{w}_k)\\
  \mathbf{z}_{k_i} &= \mathbf{g}(\mathbf{x}_{k}, \mathbf{l}_i,
    \boldsymbol{\Theta}, \mathbf{n}_k),
  \end{aligned}
\end{equation}

where

\begin{equation}\label{eqn:noise_model}
  \begin{aligned}
  \mathbf{w}_k \sim \mathcal{N}(\mathbf{0},\mathbf{Q}_k)\\
  \mathbf{n}_k \sim \mathcal{N}(\mathbf{0},\mathbf{R}_k)
  \end{aligned}
\end{equation}

are normally distributed process and observation noise variables,
$\mathbf{Q}_k$ and $\mathbf{R}_k$ being known covariance matrices. Although, the
functions $\mathbf{h}(\cdot)$ and $\mathbf{g}(\cdot)$ might be non-linear in
their parameters, we can approximate
$p(\mathbf{x}_k\mid\mathbf{x}_{k - 1},\mathbf{u}_k)$ and
$p(\mathbf{z}_{k_i}\mid\mathbf{x}_k, \mathbf{l}_i,\boldsymbol{\Theta})$ as
normal distributions through linearization.

\subsection{Least Squares Solution}
In case of linear motion and observation models, there exists a closed-form
solution to \eqref{eqn:map_estimator} based on the \emph{least squares} method
due to the normally distributed noise variables. In the other case, one can
resort to non-linear least squares methods that iteratively solve a linearized
version of the problem. In the following, we employ the \emph{Gauss-Newton}
algorithm for this purpose.

From \eqref{eqn:map_estimator} and the normal approximations, we can turn the
MAP problem into the minimization of a sum of quadratic functions. Non-linear
optimization techniques start with an initial guess
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ and refines
it iteratively with
$\delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ until
convergence.
$\delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ is
chosen in such a way that it minimizes a quadratic approximation of the
objective function around
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.
Gauss-Newton method only requires the estimation of the Jacobian matrix
$\mathbf{H}$ of the objective function. If we stack up everything in block
matrix form, the update takes the form

\begin{equation}\label{eqn:dx_update}
  \begin{aligned}
  (\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{H}^T\mathbf{T}^{-1}\mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

where $\mathbf{T}$ is formed with $\mathbf{Q}_k$ and $\mathbf{N}_k$, and
$\mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}})$ is the error of the current
estimate $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.

At convergence of the algorithm, the quantity
$\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H}$ is the \emph{Fisher Information Matrix}
(FIM) and the inverse covariance matrix
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ of
our estimate
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.

If we let $\mathbf{T}^{-1}=\mathbf{L}^T\mathbf{L}$ be the Cholesky
decomposition of the precision matrix, \eqref{eqn:dx_update} can be rewritten as

\begin{equation}\label{eqn:dx_update_normal}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})^T(\mathbf{L}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &= 
    -(\mathbf{L}\mathbf{H})^T\mathbf{e}(\mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

which we can recognize as the \emph{normal equations} of the linear system

\begin{equation}\label{eqn:dx_update_standard}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}).
  \end{aligned}
\end{equation}

Thus, instead of directly solving \eqref{eqn:dx_update} which requires a
significant amount of computation and introduces numerical errors, we can take
advantage of matrix decompositions.

Let $\mathbf{L}\mathbf{H}$ be of size $m*n$ with the following thin
Singular Value Decomposition (\emph{SVD}) decomposition

\begin{equation}\label{eqn:svd_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H} &= \mathbf{U}_n\mathbf{S}_n\mathbf{V}_n^T,
  \end{aligned}
\end{equation}

where $\mathbf{U}_n$ is $m*n$, $\mathbf{S}_n=\diag(\sigma_1, \cdots, \sigma_n)$,
$\mathbf{V}_n$ is $n*n$, and $\sigma_i$ are the singular values of
$\mathbf{L}\mathbf{H}$.

From \eqref{eqn:svd_decomposition} and using the orthogonal property of
$\mathbf{U}_n$ and $\mathbf{V}_n$, we can solve \eqref{eqn:dx_update} as

\begin{equation}\label{eqn:dx_svd_solve}
  \begin{aligned}
  \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{V}_n\mathbf{S}_n^{-1}\mathbf{U}_n^T
    \mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}).
  \end{aligned}
\end{equation}

Although useful for illustrating the concepts of our approach, the SVD
decomposition can be computationally demanding for large matrices. In practice,
we therefore use the thin \emph{QR} decomposition of $\mathbf{L}\mathbf{H}$

\begin{equation}\label{eqn:qr_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H}\boldsymbol{\Pi} &= \mathbf{Q}_n\mathbf{R}_n,
  \end{aligned}
\end{equation}

where $\boldsymbol{\Pi}$ is a permutation matrix, $\mathbf{Q}_n$ is $m*n$, and
$\mathbf{R}_n$ is $n*n$.

From \eqref{eqn:qr_decomposition} and using orthogonal property of
$\mathbf{Q}_n$, \eqref{eqn:dx_update} can be expressed as

\begin{equation}\label{eqn:dx_qr_solve}
  \begin{aligned}
  \mathbf{R}_n\boldsymbol{\Pi}^{T}
    \delta\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    -\mathbf{Q}_n^T \mathbf{e}(
    \mathbf{\hat{\boldsymbol{\mu}}_{
    \boldsymbol{\Theta}\mathcal{X}\mathcal{L}}}),
  \end{aligned}
\end{equation}

which, due the upper triangular form of $\mathbf{R}_n$ can be easily solved by
\emph{back substitution}.

The $\mathbf{R}_n$ matrix can be used to compute the FIM and its
inverse the covariance estimate
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$. If we
drop the permutation matrix for clarity, the FIM becomes
$\mathbf{R}_n^T\mathbf{R}_n$. If we let
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ having
elements $\sigma_{ij}$ and $\mathbf{R}_n$ elements $r_{ij}$, there exists an
efficient algorithm for recovering any elements of the covariance estimate
without inverting the whole FIM

\begin{equation}\label{eqn:covariance_QR}
  \begin{aligned}
  \sigma_{ll} &= \frac{1}{r_{ll}}(\frac{1}{r_{ll}} -
    \sum_{j = l + 1, r_{lj}\neq 0}^n r_{lj}\sigma_{jl})\\
  \sigma_{il} &= \frac{1}{r_{ii}}
    (-\sum_{j = i + 1, r_{ij}\neq 0}^n r_{ij}\sigma_{jl} -
    \sum_{j = l + 1, r_{ij}\neq 0}^n r_{ij}\sigma_{lj}),
  \end{aligned}
\end{equation}

for $l=n,\cdots,1$, $i=l-1,\cdots,1$ and the lower part is given by symmetry.

At the convergence of the Gauss-Newton optimization, we are thus left with the
estimates $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$
and $\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ of
the normal distribution $p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L}
\mid\mathcal{U},\mathcal{Z})$. In order to solve \eqref{eqn:post_calib}, we can
employ the marginalization property of normal
distributions~\cite{bishop06pattern}. If we express the estimates in the
partitioned form

\begin{equation}\label{eqn:partitioned_estimates}
  \begin{aligned}
  \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \begin{pmatrix}
    \hat{\boldsymbol{\mu}}_{\mathcal{X}}\\
    \hat{\boldsymbol{\mu}}_{\mathcal{L}}\\
    \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}
    \end{pmatrix},
  \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \begin{pmatrix}
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{X}\boldsymbol{\Theta}}\\
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\mathcal{L}\boldsymbol{\Theta}}\\
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}}&
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{L}}&
    \hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}\\
    \end{pmatrix},
  \end{aligned}
\end{equation}

then $p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z})\sim\mathcal{N}
(\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}},
\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}})$. We can
thus extract $\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}}$ from
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ in no time,
and by placing $\boldsymbol{\Theta}$ to the right of the Jacobian matrix
$\mathbf{H}$ and using \eqref{eqn:covariance_QR},
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\boldsymbol{\Theta}}$ can be
computed in $\mathcal{O}(l)$, where $l=\card(\boldsymbol{\Theta})$.

\subsection{Truncated SVD and QR Solutions}
The methods developed above are quite generic and yield satisfying results in
many applications. However, in practice, degenerate cases might occur when
working with real-world data.

\subsection{Selecting Informative Measurements}

