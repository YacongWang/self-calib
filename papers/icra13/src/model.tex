\subsection{Problem Formulation\label{subsec:prob}}

In the following, we outline our problem in a probabilistic manner, closely
following the discrete-time Simultaneous Localization and Mapping (SLAM)
formulation~\cite{durrantwhyte06simultaneous}. For the sake of clarity, we
consider here a robot with a single sensor observing a known number of landmarks
at each timestep. Furthermore, we assume the correspondences between sensor's
measurements and landmarks are known. The relaxation of these assumptions goes
beyond the scope of this paper.

Let
$\mathcal{X}=\{\mathbf{x}_{0:K}\}$ be a set of \emph{latent} random variables
(LRV) representing robot states up to timestep $K$,
$\mathcal{U}=\{\mathbf{u}_{1:K}\}$ a set of \emph{observable} random variables
(ORV) representing measured control inputs, $\mathcal{L}=\{\mathbf{l}_{1:N}\}$ a
set of LRV representing $N$ landmarks' positions,
$\mathcal{Z}=\{\mathbf{z}_{1_{1:N}:K_{1:N}}\}$ a set of ORV representing $KxN$
landmarks' measurements, and $\boldsymbol{\Theta}$ a LRV representing the
calibration parameters of the robot's sensor. The goal of the calibration
procedure is to compute the posterior marginal distribution of
$\boldsymbol{\Theta}$ given all the measurements up to timestep $K$

\begin{equation}\label{eqn:post_calib}
  \begin{aligned}
  p(\boldsymbol{\Theta}\mid\mathcal{U},\mathcal{Z}) &=
    \int_{\mathcal{X}, \mathcal{L}}p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

The full joint posterior on the left-hand side of \eqref{eqn:post_calib} may
further be factorized into

\begin{equation}\label{eqn:post_joint_factorized}
  \begin{aligned}
  p(\boldsymbol{\Theta}, \mathcal{X},
    \mathcal{L} \mid\mathcal{U},\mathcal{Z}) &\propto\\
    p(\boldsymbol{\Theta}, \mathbf{x}_0,\mathcal{L})
    \prod_{k=1}^K p(\mathbf{x}_k\mid\mathbf{x}_{k - 1},\mathbf{u}_k)
    \prod_{k=1}^K\prod_{i=1}^N p(\mathbf{z}_{k_i}\mid\mathbf{x}_k,
    \mathbf{l}_i,\boldsymbol{\Theta}).
  \end{aligned}
\end{equation}

We may approximate \eqref{eqn:post_joint_factorized} with a normal distribution
whose moments $\boldsymbol{\mu}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ and
$\boldsymbol{\Sigma}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$ have to be
estimated. To this end, we first derive a Maximum a Posteriori (MAP) estimator
for the mean

\begin{equation}\label{eqn:map_estimator}
  \begin{aligned}
  \hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}} &=
    \argmax_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z})
    \\
    &= \argmin_{\boldsymbol{\Theta},\mathcal{X},\mathcal{L}}-\log
    p(\boldsymbol{\Theta}, \mathcal{X},\mathcal{L} \mid\mathcal{U},\mathcal{Z}).
  \end{aligned}
\end{equation}

We further refine our problem by defining a \emph{motion} and \emph{observation}
model

\begin{equation}\label{eqn:process_model}
  \begin{aligned}
  \mathbf{x}_k &= \mathbf{h}(\mathbf{x}_{k-1}, \mathbf{u}_k, \mathbf{w}_k)\\
  \mathbf{z}_{k_i} &= \mathbf{g}(\mathbf{x}_{k}, \mathbf{l}_i, \mathbf{n}_k),
  \end{aligned}
\end{equation}

where

\begin{equation}\label{eqn:noise_model}
  \begin{aligned}
  \mathbf{w}_k \sim \mathcal{N}(\mathbf{0},\mathbf{Q}_k)\\
  \mathbf{n}_k \sim \mathcal{N}(\mathbf{0},\mathbf{R}_k)
  \end{aligned}
\end{equation}

are normally distributed process and observation noise variables,
$\mathbf{Q}_k$ and $\mathbf{R}_k$ being known covariance matrices. The functions
$\mathbf{h}(\cdot)$ and $\mathbf{g}(\cdot)$ might be non-linear in their
parameters.

\subsection{Least Squares Solution}
In case of linear motion and observation models, there exists a closed-form
solution to \eqref{eqn:map_estimator} based on the \emph{least squares} method
due to the normally distributed noise variables. In the other case, one can
resort to non-linear least squares methods that iteratively solve a linearized
version of the problem. In the following, we employ the \emph{Gauss-Newton}
algorithm for this purpose. \textit{Quick derivation of the algorithm to end up with}

\begin{equation}\label{eqn:dx_update}
  \begin{aligned}
  (\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H})\delta\mathbf{x} &=
    -\mathbf{H}^T\mathbf{T}^{-1}\mathbf{e}(\mathbf{x}).
  \end{aligned}
\end{equation}

At convergence of the algorithm, the quantity
$\mathbf{H}^T\mathbf{T}^{-1}\mathbf{H}$ is the \emph{Fisher Information Matrix}
(FIM) and the precision matrix
$\hat{\boldsymbol{\Sigma}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}^{-1}$ of
our estimate
$\hat{\boldsymbol{\mu}}_{\boldsymbol{\Theta}\mathcal{X}\mathcal{L}}$.

If we let $\mathbf{T}^{-1}=\mathbf{L}^T\mathbf{L}$ be the Cholesky
decomposition of the precision matrix, \eqref{eqn:dx_update} can be rewritten as

\begin{equation}\label{eqn:dx_update_normal}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})^T(\mathbf{L}\mathbf{H})\delta\mathbf{x} &= 
    -(\mathbf{L}\mathbf{H})^T\mathbf{e}(\mathbf{x}),
  \end{aligned}
\end{equation}

which we can recognize as the \emph{normal equations} of the linear system

\begin{equation}\label{eqn:dx_update_standard}
  \begin{aligned}
  (\mathbf{L}\mathbf{H})\delta\mathbf{x} &= -\mathbf{e}(\mathbf{x}).
  \end{aligned}
\end{equation}

Thus, instead of directly solving \eqref{eqn:dx_update} which requires a
significant amount of computation and introduces numerical errors, we can take
advantage of matrix decompositions.

Let $\mathbf{L}\mathbf{H}$ be of size $m*n$ with the following thin
Singular Value Decompostion (\emph{SVD}) decomposition

\begin{equation}\label{eqn:svd_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H} &= \mathbf{U}_n\mathbf{S}_n\mathbf{V}_n^T,
  \end{aligned}
\end{equation}

where $\mathbf{U}_n$ is $m*n$, $\mathbf{S}_n=\diag(\sigma_1, \cdots, \sigma_n)$,
$\mathbf{V}_n$ is $n*n$, and $\sigma_i$ are the singular values of
$\mathbf{L}\mathbf{H}$.

From \eqref{eqn:svd_decomposition} and using the orthogonal property of
$\mathbf{U}_n$ and $\mathbf{V}_n$, we can solve \eqref{eqn:dx_update} as

\begin{equation}\label{eqn:dx_svd_solve}
  \begin{aligned}
  \delta\mathbf{x} &= -\mathbf{V}_n\mathbf{S}_n^{-1}\mathbf{U}_n^T
    \mathbf{e}(\mathbf{x}).
  \end{aligned}
\end{equation}

Although useful for illustrating the concepts of our approach, the SVD
decomposition can be computationally demanding for large matrices. In practice,
we therefore use the thin \emph{QR} decomposition of $\mathbf{L}\mathbf{H}$

\begin{equation}\label{eqn:qr_decomposition}
  \begin{aligned}
  \mathbf{L}\mathbf{H}\boldsymbol{\Pi} &= \mathbf{Q}_n\mathbf{R}_n,
  \end{aligned}
\end{equation}

where $\boldsymbol{\Pi}$ is a permutation matrix, $\mathbf{Q}_n$ is $m*n$, and
$\mathbf{R}_n$ is $n*n$.

From \eqref{eqn:qr_decomposition} and using orthogonal property of
$\mathbf{Q}_n$, \eqref{eqn:dx_update} can be expressed as

\begin{equation}\label{eqn:dx_qr_solve}
  \begin{aligned}
  \mathbf{R}_n\boldsymbol{\Pi}^{T}\delta\mathbf{x} &=
    -\mathbf{Q}_n^T \mathbf{e}(\mathbf{x}),
  \end{aligned}
\end{equation}

which, due the upper triangular form of $\mathbf{R}_n$ can be easily solved by
\emph{back substitution}.

\subsection{Truncated QR Solution}

\subsection{Selecting Informative Measurements}

